{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from tensorflow.contrib import rnn\n",
    "import time\n",
    "import io\n",
    "import math\n",
    "import os\n",
    "from io import StringIO\n",
    "import pickle\n",
    "sample_fq = 100\n",
    "filename = 'timeserie_all_n_num.csv'\n",
    "\n",
    "start_time = time.time()\n",
    "def elapsed(sec):\n",
    "    if sec<60:\n",
    "        return str(sec) + \" sec\"\n",
    "    elif sec<(60*60):\n",
    "        return str(sec/60) + \" min\"\n",
    "    else:\n",
    "        return str(sec/(60*60)) + \" hr\"\n",
    "    \n",
    "def variable_summaries(var):\n",
    "  \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "  with tf.name_scope('summaries'):\n",
    "    mean_diff = tf.reduce_mean(var, axis = 0)    \n",
    "    \n",
    "    mean = tf.reduce_mean(mean_diff)\n",
    "    tf.summary.scalar('mean', mean)\n",
    "    with tf.name_scope('stddev'):\n",
    "        stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))        \n",
    "    tf.summary.scalar('stddev', stddev)\n",
    "print(\"done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(3, 9)\n",
      "through away something!! 249 7999 7750\n",
      "31\n",
      "(32, 250, 6) (32,)  ee  (32, 1) (32, 250, 6) (32,) (32, 1)\n"
     ]
    }
   ],
   "source": [
    "def enc(y,num = 3):\n",
    "    y_ = np.zeros((y.shape[0],3))\n",
    "    for i in range(y.shape[0]):\n",
    "        y_[i,int(y[i])] = 1\n",
    "    #print(np.sum(np.sum(y_)),np.sum(y))\n",
    "    return(y_)\n",
    "    \n",
    "    \n",
    "def reader(filename,ind,batchsize,lenght):\n",
    "    done = False\n",
    "    str_ = ''\n",
    "    with open(filename) as f:\n",
    "        \n",
    "        str_ += next(f)\n",
    "        i = 1\n",
    "        try:\n",
    "            while done is False:\n",
    "                i +=1\n",
    "\n",
    "                if i < ind:\n",
    "                    _ = next(f)\n",
    "                elif i <= ind + batchsize:\n",
    "                    str_ += next(f)\n",
    "                else:\n",
    "                    #print(\"no error during reading file i:\",i,\" ind:\",ind,\" batchsize:\",batchsize,\" length:\",lenght)\n",
    "                    done = True           \n",
    "        except:\n",
    "            print(\"Error during reading file i:\",i,\" ind:\",ind,\" batchsize:\",batchsize,\" length:\",lenght)\n",
    "            return reader(filename,int(np.floor(np.random.rand(1)*(lenght-batchsize-1))),batchsize,lenght)\n",
    "        \n",
    "        return StringIO(str_)\n",
    "class fog_data:\n",
    "    def __init__(self,filename,window_length = 250, batch_size = 32, split = [0.7,0.15,0.15]):\n",
    "        \n",
    "        self.agl = range(3,9)[:]\n",
    "        print(self.agl)\n",
    "        self.window_length = window_length\n",
    "        self.batch_size = batch_size\n",
    "        self.filename = filename\n",
    "        self.filename_training = filename[:-4] + \"_training.csv\"\n",
    "        self.filename_val = filename[:-4] + \"_val.csv\"        \n",
    "        self.filename_test = filename[:-4] + \"_test.csv\"\n",
    "        self.filename_dict = {}\n",
    "        self.len_ = {}\n",
    "        self.filename_dict[\"training\"] = self.filename_training\n",
    "        self.split = [sum(split[0:i]) for i in range(1,len(split))]\n",
    "        if not os.path.isfile(self.filename_test):\n",
    "            self.split_dataset()\n",
    "        else:\n",
    "            self.len_[\"training\"] = 9392263\n",
    "        self.filename_gan = filename[:-4] + '_gan.csv'\n",
    "        if not os.path.isfile(self.filename_gan):\n",
    "            self.makegan()\n",
    "        else:\n",
    "            self.len_[\"gan\"] = 1159500\n",
    "            self.filename_dict[\"gan\"] = self.filename_gan\n",
    "            \n",
    "            \n",
    "        self.len_[\"training_gan\"] = 64000\n",
    "        self.filename_training_gan = \"train_gan.csv\"\n",
    "        self.filename_dict[\"training_gan\"] = self.filename_training_gan\n",
    "        \n",
    "    def makegan(self):        \n",
    "\n",
    "        df = pd.read_csv(self.filename)\n",
    "        print(\"full data shape\",df.shape,np.sum(df.FOGnFOG.values))\n",
    "        \n",
    "        gandata = []\n",
    "        i = 50\n",
    "        print(df.head(),df.iloc[0:10,8])\n",
    "        while i <= (df.shape[0]-200):\n",
    "            if df.FOGnFOG[i] == 1:\n",
    "                \n",
    "                gandata.append(df.iloc[i-50:i+200,:].values)\n",
    "                i+=200\n",
    "                if i% 100 == 0:\n",
    "                    print(\"just added an extra part to the file l:\",len(gandata),\" i:\",i)\n",
    "            i += 1\n",
    "                \n",
    "        print(len(gandata))\n",
    "        np_df = np.array(gandata).reshape((-1,10))\n",
    "        print(np_df.shape,df.shape,np_df.shape[0]/df.shape[0])\n",
    "        df = pd.DataFrame(data=np_df,columns=df.columns)\n",
    "        self.len_[\"gan\"] = df.shape[0]\n",
    "        self.filename_dict[\"gan\"] = self.filename_gan\n",
    "        df.to_csv(self.filename_gan)\n",
    "             \n",
    "    def next_batch(self,type_of_data = \"training\", batchsize = 32*250):\n",
    "        \n",
    "        size = self.len_[type_of_data]\n",
    "        filen = self.filename_dict[type_of_data]\n",
    "        if type_of_data == \"training\":\n",
    "            divider = 1\n",
    "        elif type_of_data == \"gan\":\n",
    "            divider = 250\n",
    "        elif type_of_data == \"training_gan\":\n",
    "            divider = 250\n",
    "            if np.random.rand() < .5:\n",
    "                type_of_data == \"training\"\n",
    "                #print(\"going back to normal\")\n",
    "            #else:\n",
    "                #print(\"keep beeing weird\")\n",
    "        \n",
    "        ind = int(np.floor(np.random.rand(1)*(size-batchsize)/divider))\n",
    "        df = pd.read_csv(reader(filen,ind*divider,batchsize,size))\n",
    "        \n",
    "        return self.return_data(df)\n",
    "    \n",
    "    def val(self):\n",
    "        df = pd.read_csv(self.filename_val)\n",
    "        return self.return_data(df,train = True)\n",
    "    def test(self):\n",
    "        df = pd.read_csv(self.filename_test)\n",
    "        return self.return_data(df,train = True)\n",
    "    \n",
    "\n",
    "    def split_dataset(self):\n",
    "        \n",
    "        filenames = [self.filename_training,self.filename_val,self.filename_test]\n",
    "        with open(self.filename,'r') as f:\n",
    "            self.length = sum(1 for _ in f)\n",
    "        with open(self.filename,'r') as f:\n",
    "            self.split.append(self.length+1)\n",
    "            self.split_i = np.floor([self.length * s for s in self.split])\n",
    "            print(self.split_i)\n",
    "            s = 0\n",
    "            i = 0\n",
    "            file = open(filenames[s],'w')\n",
    "            for line in f:\n",
    "                if i >= self.split_i[s]:\n",
    "                    file.close()\n",
    "                    s += 1\n",
    "                    file = open(filenames[s],'w')\n",
    "                    file.write(head)\n",
    "                    print(\"switch to \",filenames[s])\n",
    "                \n",
    "                if i ==0:\n",
    "                    head = line\n",
    "                                        \n",
    "                file.write(line)\n",
    "                i +=1\n",
    "                #print(i)\n",
    "                \n",
    "            file.close()\n",
    "            print(\"splitted the data in different files according to \", [0, self.split_i, self.length])\n",
    "            self.len_ = {}\n",
    "            13417517\n",
    "            self.len_[\"training\"] = self.split_i[0]\n",
    "            self.len_[\"val\"], self.len_[\"test\"] =  self.split_i[1] - self.split_i[0], self.length - self.split_i[1]\n",
    "            \n",
    "    def return_data(self,df,train = False): # include self.window_size\n",
    "        acc_gyr = df.iloc[:,self.agl].values\n",
    "        acc_gyr = (acc_gyr-np.mean(acc_gyr))/np.std(acc_gyr)\n",
    "        num_windows = int(np.floor(np.divide(acc_gyr.shape[0],(self.window_length))))\n",
    "        if acc_gyr.shape[0] - num_windows*self.window_length >1:\n",
    "            print(\"through away something!!\",acc_gyr.shape[0] - num_windows*self.window_length,acc_gyr.shape[0], num_windows*self.window_length)\n",
    "            print(num_windows)\n",
    "        a_g  = acc_gyr[:num_windows*self.window_length,:].reshape(num_windows,self.window_length,6)\n",
    "        \n",
    "        fog = df.FOGnFOG[:num_windows*self.window_length].values\n",
    "        fog_reshape = fog.reshape(num_windows,self.window_length,1)\n",
    "        fognfog = np.mean(fog_reshape,axis=1)\n",
    "        if np.random.rand() < 0.95 and train:\n",
    "            inx = np.random.randint(0,high=a_g.shape[0])\n",
    "            a_g[inx,:,:] = (np.random.rand(a_g[inx,:,:].shape[0],a_g[inx,:,:].shape[1])*2-1)*a_g[inx,:,:]\n",
    "            fognfog[inx,:] = 2\n",
    "        \n",
    "        \n",
    "        return a_g, np.round(fognfog[:,0]), fognfog\n",
    "    \n",
    "\n",
    "fog = fog_data(filename)\n",
    "acc, fog_, fogi = fog.next_batch()\n",
    "a, f, fi = fog.next_batch(\"gan\")\n",
    "for _ in range(10):\n",
    "    a, f, fi = fog.next_batch(\"training_gan\")\n",
    "print(acc.shape, fog_.shape,\" ee \", fogi.shape ,a.shape,f.shape,fi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n",
      "current run time:  248.0745575428009 250\n",
      "through away something!! 128 2012628 2012500\n",
      "8050\n",
      "current run time:  517.0489251613617 500\n",
      "through away something!! 128 2012628 2012500\n",
      "8050\n",
      "current run time:  801.9047815799713 750\n",
      "through away something!! 128 2012628 2012500\n",
      "8050\n",
      "current run time:  1086.5213084220886 1000\n",
      "through away something!! 128 2012628 2012500\n",
      "8050\n",
      "current run time:  1362.0577828884125 1250\n",
      "through away something!! 128 2012628 2012500\n",
      "8050\n",
      "current run time:  1649.4941756725311 1500\n",
      "through away something!! 128 2012628 2012500\n",
      "8050\n",
      "current run time:  1913.9019417762756 1750\n",
      "through away something!! 128 2012628 2012500\n",
      "8050\n",
      "current run time:  2182.68434381485 2000\n",
      "through away something!! 128 2012628 2012500\n",
      "8050\n",
      "current run time:  2454.952432155609 2250\n",
      "through away something!! 128 2012628 2012500\n",
      "8050\n",
      "current run time:  2739.6069507598877 2500\n",
      "through away something!! 128 2012628 2012500\n",
      "8050\n",
      "current run time:  3010.420750141144 2750\n",
      "through away something!! 128 2012628 2012500\n",
      "8050\n",
      "current run time:  3278.847240924835 3000\n",
      "through away something!! 128 2012628 2012500\n",
      "8050\n",
      "current run time:  3550.6866421699524 3250\n",
      "through away something!! 128 2012628 2012500\n",
      "8050\n",
      "current run time:  3811.8405442237854 3500\n",
      "through away something!! 128 2012628 2012500\n",
      "8050\n",
      "current run time:  4074.8258068561554 3750\n",
      "through away something!! 128 2012628 2012500\n",
      "8050\n",
      "current run time:  4351.967682123184 4000\n",
      "through away something!! 128 2012628 2012500\n",
      "8050\n",
      "current run time:  4627.395967006683 4250\n",
      "through away something!! 128 2012628 2012500\n",
      "8050\n",
      "current run time:  4924.236541748047 4500\n",
      "through away something!! 128 2012628 2012500\n",
      "8050\n",
      "current run time:  5202.5396819114685 4750\n",
      "through away something!! 128 2012628 2012500\n",
      "8050\n",
      "current run time:  5467.473898649216 5000\n",
      "through away something!! 128 2012628 2012500\n",
      "8050\n",
      "through away something!! 128 2012628 2012500\n",
      "8050\n",
      "through away something!! 128 2012628 2012500\n",
      "8050\n"
     ]
    }
   ],
   "source": [
    "mb_size = 32\n",
    "X_dim = 250* 6\n",
    "h_dim = 512\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None,X_dim])\n",
    "Y = tf.placeholder(tf.int32, shape = [None])\n",
    "\n",
    "D_W1 = tf.Variable(xavier_init([X_dim, h_dim]))\n",
    "D_b1 = tf.Variable(tf.zeros(shape=[h_dim]))\n",
    "\n",
    "D_W2 = tf.Variable(xavier_init([h_dim, 3]))\n",
    "D_b2 = tf.Variable(tf.zeros(shape=[3]))\n",
    "\n",
    "def d(x):\n",
    "    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)\n",
    "    out = tf.matmul(D_h1, D_W2) + D_b2\n",
    "    return tf.nn.softmax(out)\n",
    "\n",
    "X_batch = tf.contrib.layers.batch_norm(X)\n",
    "\n",
    "end = d(X_batch)\n",
    "\"\"\"weighted_cross_entropy_with_logits(\n",
    "    targets,\n",
    "    logits,\n",
    "    pos_weight,\n",
    "    name=None\n",
    ")\"\"\"\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.sparse_softmax_cross_entropy_with_logits(labels = Y, logits = end))\n",
    "tf.summary.scalar('mini_batch_cross_entropy', cross_entropy)\n",
    "optimize = tf.train.AdamOptimizer(learning_rate = 1e-4).minimize(cross_entropy)\n",
    "\n",
    "\n",
    "y_pred = tf.argmax(end,1)\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "model_name = \"FOG_512_2sd_try\"\n",
    "train_writer = tf.summary.FileWriter(\"tensorboard_FOG\" + '/train/model_' + model_name)\n",
    "val_writer = tf.summary.FileWriter(\"tensorboard_FOG\" + '/val/model_' + model_name)\n",
    "saver = tf.train.Saver( tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES))\n",
    "\n",
    "print(\"start training\")\n",
    "start_time =time.time()\n",
    "# Initializing the variables\n",
    "b_size = 32\n",
    "init = tf.global_variables_initializer()\n",
    "i = 0\n",
    "# Launch the graph\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    start_training_time = time.time()\n",
    "    while i < 5000:\n",
    "        i+=1\n",
    "        X_mb, input_y, _ = fog.next_batch(batchsize = b_size*250)\n",
    "        input_x = X_mb.reshape((-1,250*6))\n",
    "        _, _summary = session.run([optimize, merged], feed_dict={X: input_x, Y: input_y})\n",
    "        train_writer.add_summary(_summary, i * b_size )\n",
    "    \n",
    "        if i%250 is 0 and i is not 0: \n",
    "            print(\"current run time: \",time.time()-start_time,i)\n",
    "            X_mb, input_y, _ = fog.val()       \n",
    "            input_x = X_mb.reshape((-1,250*6))\n",
    "            _, _summary = session.run([cross_entropy, merged ], feed_dict={X: input_x, Y: input_y})\n",
    "            val_writer.add_summary(_summary, i * b_size )\n",
    "    X_mb, input_y, _ = fog.val()       \n",
    "    input_x = X_mb.reshape((-1,250*6))\n",
    "    y_test = session.run([y_pred ], feed_dict={X: input_x, Y: input_y})\n",
    "    pickle.dump([y_test,input_y],open('yval_normal.pickle','wb'))\n",
    "    \n",
    "    X_mb, input_y, _ = fog.test()       \n",
    "    input_x = X_mb.reshape((-1,250*6))\n",
    "    y_test = session.run([y_pred ], feed_dict={X: input_x, Y: input_y})\n",
    "    pickle.dump([y_test,input_y],open('ytest_normal.pickle','wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'gan_training'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b302a21a7711>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mX_mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_of_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"gan_training\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatchsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0minput_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-3c8d64630849>\u001b[0m in \u001b[0;36mnext_batch\u001b[0;34m(self, type_of_data, batchsize)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtype_of_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"training\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype_of_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mfilen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype_of_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype_of_data\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"training\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'gan_training'"
     ]
    }
   ],
   "source": [
    "mb_size = 32\n",
    "X_dim = 250* 6\n",
    "h_dim = 512\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None,X_dim])\n",
    "Y = tf.placeholder(tf.int32, shape = [None])\n",
    "\n",
    "D_W1 = tf.Variable(xavier_init([X_dim, h_dim]))\n",
    "D_b1 = tf.Variable(tf.zeros(shape=[h_dim]))\n",
    "\n",
    "D_W2 = tf.Variable(xavier_init([h_dim, 3]))\n",
    "D_b2 = tf.Variable(tf.zeros(shape=[3]))\n",
    "\n",
    "def d(x):\n",
    "    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)\n",
    "    out = tf.matmul(D_h1, D_W2) + D_b2\n",
    "    return tf.nn.softmax(out)\n",
    "\n",
    "X_batch = tf.contrib.layers.batch_norm(X)\n",
    "\n",
    "end = d(X_batch)\n",
    "\"\"\"weighted_cross_entropy_with_logits(\n",
    "    targets,\n",
    "    logits,\n",
    "    pos_weight,\n",
    "    name=None\n",
    ")\"\"\"\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.sparse_softmax_cross_entropy_with_logits(labels = Y, logits = end))\n",
    "tf.summary.scalar('mini_batch_cross_entropy', cross_entropy)\n",
    "optimize = tf.train.AdamOptimizer(learning_rate = 1e-4).minimize(cross_entropy)\n",
    "\n",
    "\n",
    "y_pred = tf.argmax(end,1)\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "model_name = \"FOG_512_gan\"\n",
    "train_writer = tf.summary.FileWriter(\"tensorboard_FOG\" + '/train/model_1080_' + model_name)\n",
    "val_writer = tf.summary.FileWriter(\"tensorboard_FOG\" + '/val/model_1080_' + model_name)\n",
    "saver = tf.train.Saver( tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES))\n",
    "\n",
    "print(\"start training\")\n",
    "start_time =time.time()\n",
    "# Initializing the variables\n",
    "b_size = 32\n",
    "init = tf.global_variables_initializer()\n",
    "i = 0\n",
    "# Launch the graph\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    start_training_time = time.time()\n",
    "    while i < 5000:\n",
    "        i+=1\n",
    "        X_mb, input_y, _ = fog.next_batch(type_of_data = \"gan_training\",batchsize = b_size*250)\n",
    "        input_x = X_mb.reshape((-1,250*6))\n",
    "        _, _summary = session.run([optimize, merged], feed_dict={X: input_x, Y: input_y})\n",
    "        train_writer.add_summary(_summary, i * b_size )\n",
    "    \n",
    "        if i%250 is 0 and i is not 0: \n",
    "            print(\"current run time: \",time.time()-start_time,i)\n",
    "            X_mb, input_y, _ = fog.val()       \n",
    "            input_x = X_mb.reshape((-1,250*6))\n",
    "            _, _summary = session.run([cross_entropy, merged ], feed_dict={X: input_x, Y: input_y})\n",
    "            val_writer.add_summary(_summary, i * b_size )\n",
    "    \n",
    "    X_mb, input_y, _ = fog.val()       \n",
    "    input_x = X_mb.reshape((-1,250*6))\n",
    "    y_test = session.run([y_pred ], feed_dict={X: input_x, Y: input_y})\n",
    "    pickle.dump([y_test,input_y],open('yval_gan.pickle','wb'))        \n",
    "            \n",
    "    X_mb, input_y, _ = fog.test()       \n",
    "    input_x = X_mb.reshape((-1,250*6))\n",
    "    y_test = session.run([y_pred ], feed_dict={X: input_x, Y: input_y})\n",
    "    pickle.dump([y_test,input_y],open('ytest_gan.pickle','wb'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
