{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "#from tensorflow.contrib import rnn\n",
    "from io import StringIO\n",
    "filename = 'timeserie_all_n_num.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def reader(filename,ind,batchsize,lenght):\n",
    "    done = False\n",
    "    str_ = ''\n",
    "    with open(filename) as f:\n",
    "        \n",
    "        str_ += next(f)\n",
    "        i = 1\n",
    "        try:\n",
    "            while done is False:\n",
    "                i +=1\n",
    "\n",
    "                if i < ind:\n",
    "                    _ = next(f)\n",
    "                elif i <= ind + batchsize:\n",
    "                    str_ += next(f)\n",
    "                else:\n",
    "                    #print(\"no error during reading file i:\",i,\" ind:\",ind,\" batchsize:\",batchsize,\" length:\",lenght)\n",
    "                    done = True           \n",
    "        except:\n",
    "            print(\"Error during reading file i:\",i,\" ind:\",ind,\" batchsize:\",batchsize,\" length:\",lenght)\n",
    "        \n",
    "        return StringIO(str_)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(3, 9)\n",
      "Index(['Unnamed: 0', 'Time', 'Accelerometer_1', 'Accelerometer_2',\n",
      "       'Accelerometer_3', 'Gyroscoop_1', 'Gyroscoop_2', 'Gyroscoop_3',\n",
      "       'FOGnFOG', 'N_num'],\n",
      "      dtype='object')\n",
      "Index(['Unnamed: 0', 'Unnamed: 0.1', 'Time', 'Accelerometer_1',\n",
      "       'Accelerometer_2', 'Accelerometer_3', 'Gyroscoop_1', 'Gyroscoop_2',\n",
      "       'Gyroscoop_3', 'FOGnFOG', 'N_num'],\n",
      "      dtype='object')\n",
      "(32, 250, 6) (32, 1, 1) (32, 250, 6) (32, 1, 1) (32, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "class fog_data:\n",
    "    def __init__(self,filename,window_length = 250, batch_size = 32, split = [0.7,0.15,0.15]):\n",
    "        \n",
    "        self.agl = range(3,9)[:]\n",
    "        print(self.agl)\n",
    "        self.window_length = window_length\n",
    "        self.batch_size = batch_size\n",
    "        self.filename = filename\n",
    "        self.filename_training = filename[:-4] + \"_training.csv\"\n",
    "        self.filename_val = filename[:-4] + \"_val.csv\"        \n",
    "        self.filename_test = filename[:-4] + \"_test.csv\"\n",
    "        self.filename_dict = {}\n",
    "        self.len_ = {}\n",
    "        self.filename_dict[\"training\"] = self.filename_training\n",
    "        self.split = [sum(split[0:i]) for i in range(1,len(split))]\n",
    "        if not os.path.isfile(self.filename_test):\n",
    "            self.split_dataset()\n",
    "        else:\n",
    "            self.len_[\"training\"] = 9392263\n",
    "        self.filename_gan = filename[:-4] + '_gan.csv'\n",
    "        if not os.path.isfile(self.filename_gan):\n",
    "            self.makegan()\n",
    "        else:\n",
    "            self.len_[\"gan\"] = 1159500\n",
    "            self.filename_dict[\"gan\"] = self.filename_gan\n",
    "        \n",
    "        \n",
    "    def makegan(self):        \n",
    "\n",
    "        df = pd.read_csv(self.filename)\n",
    "        print(\"full data shape\",df.shape,np.sum(df.FOGnFOG.values))\n",
    "        \n",
    "        gandata = []\n",
    "        i = 50\n",
    "        print(df.head(),df.iloc[0:10,8])\n",
    "        while i <= (df.shape[0]-200):\n",
    "            if df.FOGnFOG[i] == 1:\n",
    "                #print(df.iloc[i-50:i+200,:].values.shape)\n",
    "                gandata.append(df.iloc[i-50:i+200,:].values)\n",
    "                i+=200\n",
    "                if i% 100 == 0:\n",
    "                    print(\"just added an extra part to the file l:\",len(gandata),\" i:\",i)\n",
    "            i += 1\n",
    "            \n",
    "        print(len(gandata))\n",
    "        np_df = np.array(gandata).reshape((-1,10))\n",
    "        print(np_df.shape,df.shape,np_df.shape[0]/df.shape[0])\n",
    "        df = pd.DataFrame(data=np_df,columns=df.columns)\n",
    "        self.len_[\"gan\"] = df.shape[0]\n",
    "        self.filename_dict[\"gan\"] = self.filename_gan\n",
    "        df.to_csv(self.filename_gan)\n",
    "             \n",
    "    def next_batch(self,type_of_data = \"training\", batchsize = 32*250):\n",
    "        \n",
    "        size = self.len_[type_of_data]\n",
    "        if type_of_data == \"gan\":\n",
    "            divider = 250\n",
    "        else:\n",
    "            divider = 1\n",
    "        ind = int(np.floor(np.random.rand(1)*(size-batchsize)/divider))\n",
    "        df = pd.read_csv(reader(self.filename_dict[type_of_data],ind*divider,batchsize,size))\n",
    "        print(df.columns)\n",
    "        return self.return_data(df)\n",
    "    \n",
    "    def val(self):\n",
    "        df = pd.read_csv(self.filename_val)\n",
    "        return self.return_data(df)\n",
    "    def test(self):\n",
    "        df = pd.read_csv(self.filename_test)\n",
    "        return self.return_data(df)\n",
    "    \n",
    "\n",
    "    def split_dataset(self):\n",
    "        \n",
    "        filenames = [self.filename_training,self.filename_val,self.filename_test]\n",
    "        with open(self.filename,'r') as f:\n",
    "            self.length = sum(1 for _ in f)\n",
    "        with open(self.filename,'r') as f:\n",
    "            self.split.append(self.length+1)\n",
    "            self.split_i = np.floor([self.length * s for s in self.split])\n",
    "            print(self.split_i)\n",
    "            s = 0\n",
    "            i = 0\n",
    "            file = open(filenames[s],'w')\n",
    "            for line in f:\n",
    "                if i >= self.split_i[s]:\n",
    "                    file.close()\n",
    "                    s += 1\n",
    "                    file = open(filenames[s],'w')\n",
    "                    file.write(head)\n",
    "                    print(\"switch to \",filenames[s])\n",
    "                \n",
    "                if i ==0:\n",
    "                    head = line\n",
    "                                        \n",
    "                file.write(line)\n",
    "                i +=1\n",
    "                #print(i)\n",
    "                \n",
    "            file.close()\n",
    "            print(\"splitted the data in different files according to \", [0, self.split_i, self.length])\n",
    "            self.len_ = {}\n",
    "            13417517\n",
    "            self.len_[\"training\"] = self.split_i[0]\n",
    "            self.len_[\"val\"], self.len_[\"test\"] =  self.split_i[1] - self.split_i[0], self.length - self.split_i[1]\n",
    "            \n",
    "    def return_data(self,df): # include self.window_size\n",
    "        acc_gyr = df.iloc[:,self.agl].values\n",
    "        acc_gyr = (acc_gyr-np.mean(acc_gyr))/np.std(acc_gyr)\n",
    "        num_windows = int(np.floor(np.divide(acc_gyr.shape[0],(self.window_length))))\n",
    "        if acc_gyr.shape[0] - num_windows*self.window_length >1:\n",
    "            print(\"through away something!!\",acc_gyr.shape[0] - num_windows*self.window_length,acc_gyr.shape[0], num_windows*self.window_length)\n",
    "            print(num_windows)\n",
    "        a_g  = acc_gyr[:num_windows*self.window_length,:].reshape(num_windows,self.window_length,6)\n",
    "        \n",
    "        fog = df.iloc[:num_windows*self.window_length,7].values\n",
    "        fog_reshape = fog.reshape(num_windows,self.window_length,1)\n",
    "        fognfog = np.mean(fog_reshape,axis=1,keepdims=True)\n",
    "        return a_g, np.round(fognfog), fognfog\n",
    "              \n",
    "\n",
    "fog = fog_data(filename)\n",
    "acc, fog_, fogi = fog.next_batch()\n",
    "\n",
    "a, f, fi = fog.next_batch(\"gan\")\n",
    "print(acc.shape, fogi.shape,a.shape,f.shape,fi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 2 *512\n",
      "starting time: 1511948167.6277127\n",
      "Iter: 0; D loss: -0.02077; G_loss: -0.02529\n",
      "current run time: 0.6736915111541748\n",
      "Iter: 100; D loss: -12.79; G_loss: 0.2572\n",
      "Iter: 200; D loss: 0.5361; G_loss: -1.749\n",
      "Iter: 300; D loss: -0.3359; G_loss: -1.812\n",
      "Iter: 400; D loss: -0.7971; G_loss: -0.009681\n",
      "through away something!! 249 7999 7750\n",
      "31\n",
      "Iter: 500; D loss: -4.063; G_loss: 1.405\n",
      "Iter: 600; D loss: -1.519; G_loss: -0.2511\n",
      "Iter: 700; D loss: -2.224; G_loss: 0.1051\n",
      "through away something!! 249 7999 7750\n",
      "31\n",
      "Iter: 800; D loss: -0.5421; G_loss: 0.01813\n",
      "Iter: 900; D loss: -1.038; G_loss: 0.2235\n",
      "Iter: 1000; D loss: -0.7857; G_loss: 0.5837\n",
      "current run time: 826.297043800354\n",
      "through away something!! 249 7999 7750\n",
      "31\n",
      "Iter: 1100; D loss: -0.9108; G_loss: 0.3608\n",
      "Iter: 1200; D loss: -0.8993; G_loss: 0.2702\n",
      "Iter: 1300; D loss: -0.7586; G_loss: -0.01563\n",
      "Iter: 1400; D loss: -0.4002; G_loss: -0.2064\n",
      "Iter: 1500; D loss: -1.059; G_loss: 0.2085\n",
      "through away something!! 249 7999 7750\n",
      "31\n",
      "Iter: 1600; D loss: -1.075; G_loss: 0.05088\n",
      "Iter: 1700; D loss: -0.6792; G_loss: -0.04478\n",
      "Iter: 1800; D loss: -1.262; G_loss: 0.1057\n",
      "Iter: 1900; D loss: -0.6103; G_loss: -0.28\n",
      "Iter: 2000; D loss: -0.2659; G_loss: 0.2829\n",
      "current run time: 1656.3445658683777\n",
      "Iter: 2100; D loss: -1.023; G_loss: 0.1464\n",
      "Iter: 2200; D loss: -0.4829; G_loss: -0.1816\n",
      "Iter: 2300; D loss: -0.5191; G_loss: -0.3862\n",
      "Iter: 2400; D loss: -1.048; G_loss: 0.4373\n",
      "Iter: 2500; D loss: -0.4823; G_loss: -0.4212\n",
      "Iter: 2600; D loss: -0.6322; G_loss: -0.1003\n",
      "Iter: 2700; D loss: -0.6893; G_loss: -0.759\n",
      "Iter: 2800; D loss: -0.1399; G_loss: -0.09012\n",
      "Iter: 2900; D loss: -0.3929; G_loss: -0.3093\n",
      "Iter: 3000; D loss: -0.2418; G_loss: -0.3163\n",
      "current run time: 2494.978406906128\n",
      "Iter: 3100; D loss: 0.09291; G_loss: 0.06047\n",
      "Iter: 3200; D loss: -0.837; G_loss: 0.5213\n",
      "Iter: 3300; D loss: -0.8734; G_loss: -0.3619\n",
      "Iter: 3400; D loss: -0.3282; G_loss: 0.00195\n",
      "Iter: 3500; D loss: -0.4147; G_loss: 0.006524\n",
      "Iter: 3600; D loss: -0.6793; G_loss: -0.07089\n",
      "Iter: 3700; D loss: -0.524; G_loss: 0.1105\n",
      "Iter: 3800; D loss: -0.2104; G_loss: -0.428\n",
      "Iter: 3900; D loss: -0.7185; G_loss: 0.02983\n",
      "Iter: 4000; D loss: -0.07266; G_loss: 0.01287\n",
      "current run time: 3342.005999326706\n",
      "Iter: 4100; D loss: -0.6769; G_loss: -0.05767\n",
      "Iter: 4200; D loss: -0.4234; G_loss: -0.1805\n",
      "Iter: 4300; D loss: 0.05219; G_loss: 0.6176\n",
      "Iter: 4400; D loss: -0.2807; G_loss: 0.4848\n",
      "Iter: 4500; D loss: -0.5696; G_loss: 0.3247\n",
      "Iter: 4600; D loss: -0.7102; G_loss: 0.3855\n",
      "Iter: 4700; D loss: -0.8808; G_loss: -0.4044\n",
      "Iter: 4800; D loss: -1.068; G_loss: 1.081\n",
      "Iter: 4900; D loss: -0.5409; G_loss: -0.05509\n",
      "Iter: 5000; D loss: -0.5699; G_loss: -0.1291\n",
      "current run time: 4194.713933706284\n",
      "Iter: 5100; D loss: -0.2464; G_loss: -0.06822\n",
      "Iter: 5200; D loss: -1.063; G_loss: 0.6597\n",
      "Iter: 5300; D loss: -0.1828; G_loss: -0.5587\n",
      "Iter: 5400; D loss: -0.5226; G_loss: -0.3044\n",
      "Iter: 5500; D loss: -0.467; G_loss: -0.3272\n",
      "Iter: 5600; D loss: -0.3349; G_loss: 0.2366\n",
      "Iter: 5700; D loss: -0.867; G_loss: -0.7359\n",
      "Iter: 5800; D loss: -0.3767; G_loss: -0.3812\n",
      "Iter: 5900; D loss: -1.222; G_loss: 1.02\n",
      "Iter: 6000; D loss: -0.3329; G_loss: 1.25\n",
      "current run time: 5104.680637598038\n",
      "Iter: 6100; D loss: -0.8987; G_loss: 0.2579\n",
      "through away something!! 249 7999 7750\n",
      "31\n",
      "Iter: 6200; D loss: -0.7976; G_loss: 0.5561\n",
      "Iter: 6300; D loss: -0.6659; G_loss: 0.1039\n",
      "Iter: 6400; D loss: 0.1671; G_loss: 0.9094\n",
      "Iter: 6500; D loss: -0.7467; G_loss: 0.1209\n",
      "Iter: 6600; D loss: -0.5063; G_loss: -0.611\n",
      "Iter: 6700; D loss: -0.7637; G_loss: -0.46\n",
      "Iter: 6800; D loss: -0.3942; G_loss: 1.073\n",
      "Iter: 6900; D loss: -0.8769; G_loss: -0.6865\n",
      "Iter: 7000; D loss: -0.672; G_loss: -0.4857\n",
      "current run time: 5933.169517278671\n",
      "Iter: 7100; D loss: -0.5225; G_loss: -0.406\n",
      "Iter: 7200; D loss: -0.6123; G_loss: -1.595\n",
      "Iter: 7300; D loss: -1.071; G_loss: 0.4378\n",
      "through away something!! 249 7999 7750\n",
      "31\n",
      "Iter: 7400; D loss: -0.2446; G_loss: -0.6515\n",
      "Iter: 7500; D loss: -0.7837; G_loss: 0.2314\n",
      "Iter: 7600; D loss: -0.2742; G_loss: -0.121\n",
      "Iter: 7700; D loss: -0.5896; G_loss: -0.9529\n",
      "Iter: 7800; D loss: -0.6915; G_loss: -0.3549\n",
      "Iter: 7900; D loss: 0.005577; G_loss: -0.8753\n",
      "Iter: 8000; D loss: -0.7626; G_loss: -0.1198\n",
      "current run time: 6769.67512345314\n",
      "Iter: 8100; D loss: -0.3266; G_loss: -0.2014\n",
      "Iter: 8200; D loss: -0.706; G_loss: -0.3727\n",
      "Iter: 8300; D loss: 0.01669; G_loss: 0.7918\n",
      "Iter: 8400; D loss: -0.1586; G_loss: -1.308\n",
      "Iter: 8500; D loss: -0.4613; G_loss: -0.1772\n",
      "Iter: 8600; D loss: -0.6219; G_loss: -0.9346\n",
      "through away something!! 249 7999 7750\n",
      "31\n",
      "Iter: 8700; D loss: -1.276; G_loss: -2.574\n",
      "through away something!! 249 7999 7750\n",
      "31\n",
      "Iter: 8800; D loss: -0.01253; G_loss: 0.02178\n",
      "Iter: 8900; D loss: -0.1794; G_loss: -0.06692\n",
      "through away something!! 249 7999 7750\n",
      "31\n",
      "Iter: 9000; D loss: -0.529; G_loss: 0.9672\n",
      "current run time: 7647.4002203941345\n",
      "Iter: 9100; D loss: 0.2767; G_loss: 1.199\n",
      "Iter: 9200; D loss: -0.6677; G_loss: -0.1382\n",
      "Iter: 9300; D loss: -0.1359; G_loss: 0.02303\n",
      "Iter: 9400; D loss: -0.2116; G_loss: -0.1424\n",
      "Iter: 9500; D loss: -0.3148; G_loss: 0.2262\n",
      "through away something!! 249 7999 7750\n",
      "31\n",
      "Iter: 9600; D loss: -0.4627; G_loss: -0.2732\n",
      "Iter: 9700; D loss: -0.2813; G_loss: -0.04795\n",
      "Iter: 9800; D loss: -0.7869; G_loss: -0.8674\n",
      "Iter: 9900; D loss: -0.2456; G_loss: -0.3315\n",
      "Iter: 10000; D loss: -0.5115; G_loss: -0.08792\n",
      "current run time: 8525.489830970764\n",
      "through away something!! 249 7999 7750\n",
      "31\n",
      "Iter: 10100; D loss: -0.2805; G_loss: 0.1784\n",
      "Iter: 10200; D loss: -0.7161; G_loss: -0.8473\n",
      "Iter: 10300; D loss: -0.1103; G_loss: -0.4697\n",
      "Iter: 10400; D loss: -1.145; G_loss: -1.184\n",
      "Iter: 10500; D loss: -0.8626; G_loss: 0.6998\n",
      "through away something!! 249 7999 7750\n",
      "31\n",
      "Iter: 10600; D loss: -0.8437; G_loss: -1.054\n",
      "Iter: 10700; D loss: -0.4213; G_loss: -0.6004\n",
      "Iter: 10800; D loss: -0.07058; G_loss: 0.5642\n",
      "Iter: 10900; D loss: -1.174; G_loss: -0.5809\n",
      "Iter: 11000; D loss: -0.5532; G_loss: -0.1442\n",
      "current run time: 9349.746702194214\n",
      "Iter: 11100; D loss: -1.001; G_loss: -0.6817\n",
      "Iter: 11200; D loss: 0.1451; G_loss: 0.6049\n",
      "Iter: 11300; D loss: -0.4713; G_loss: 1.017\n",
      "Iter: 11400; D loss: -0.1807; G_loss: 1.039\n",
      "Iter: 11500; D loss: 0.3154; G_loss: -0.9422\n",
      "Iter: 11600; D loss: -0.4031; G_loss: -0.438\n",
      "Iter: 11700; D loss: -0.3532; G_loss: -0.09606\n",
      "Iter: 11800; D loss: -0.7452; G_loss: 0.7795\n",
      "Iter: 11900; D loss: -0.9168; G_loss: -0.8909\n",
      "Iter: 12000; D loss: -0.0758; G_loss: 0.3168\n",
      "current run time: 10236.369090557098\n",
      "through away something!! 249 7999 7750\n",
      "31\n",
      "Iter: 12100; D loss: -0.03922; G_loss: 0.5093\n",
      "Iter: 12200; D loss: -0.2685; G_loss: 0.9831\n",
      "Iter: 12300; D loss: -0.4124; G_loss: 0.2638\n",
      "Iter: 12400; D loss: -0.8272; G_loss: 0.4627\n",
      "Iter: 12500; D loss: -0.6161; G_loss: 0.8029\n",
      "Iter: 12600; D loss: -0.3558; G_loss: -0.09912\n",
      "Iter: 12700; D loss: -0.5256; G_loss: 0.2899\n",
      "Iter: 12800; D loss: -0.241; G_loss: -0.0515\n",
      "Iter: 12900; D loss: -0.5885; G_loss: -0.4862\n",
      "Iter: 13000; D loss: -0.3634; G_loss: -0.0756\n",
      "current run time: 11178.095941066742\n",
      "Iter: 13100; D loss: -0.7804; G_loss: 0.9481\n",
      "Iter: 13200; D loss: -0.2186; G_loss: 0.5438\n",
      "Iter: 13300; D loss: -0.393; G_loss: -0.5253\n",
      "Iter: 13400; D loss: -0.4028; G_loss: 0.3922\n",
      "Iter: 13500; D loss: -0.783; G_loss: -0.735\n",
      "Iter: 13600; D loss: -0.1789; G_loss: -0.4218\n",
      "Iter: 13700; D loss: -0.4136; G_loss: -0.04336\n",
      "Iter: 13800; D loss: 0.2464; G_loss: -0.8335\n",
      "Iter: 13900; D loss: -0.8066; G_loss: -0.4604\n",
      "Iter: 14000; D loss: 0.1875; G_loss: -0.7321\n",
      "current run time: 12139.43552184105\n",
      "Iter: 14100; D loss: -0.4386; G_loss: -0.1262\n",
      "Iter: 14200; D loss: -0.4012; G_loss: 0.04911\n",
      "Iter: 14300; D loss: -0.2935; G_loss: -0.284\n",
      "Iter: 14400; D loss: -0.2831; G_loss: -0.2626\n",
      "Iter: 14500; D loss: -0.4525; G_loss: 0.04132\n",
      "Iter: 14600; D loss: -1.078; G_loss: 0.6824\n",
      "Iter: 14700; D loss: -0.3209; G_loss: 0.217\n",
      "Iter: 14800; D loss: -0.3364; G_loss: -1.502\n",
      "Iter: 14900; D loss: -0.2637; G_loss: -0.1407\n",
      "Iter: 15000; D loss: -0.8589; G_loss: 0.8417\n",
      "current run time: 12962.330247402191\n",
      "Iter: 15100; D loss: -0.7703; G_loss: -1.147\n",
      "Iter: 15200; D loss: -0.2037; G_loss: 0.312\n",
      "Iter: 15300; D loss: -0.8526; G_loss: -0.3922\n",
      "Iter: 15400; D loss: -0.7787; G_loss: -0.6195\n",
      "Iter: 15500; D loss: -0.9792; G_loss: -0.5768\n"
     ]
    }
   ],
   "source": [
    "mb_size = 32\n",
    "X_dim = 250 * 6\n",
    "z_dim = 5\n",
    "h_dim = 512\n",
    "lam = 10\n",
    "n_disc = 5\n",
    "lr = 1e-4\n",
    "\n",
    "model_name = str(z_dim) + \" 2 *\" + str(h_dim)\n",
    "print(model_name)\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, X_dim])\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, X_dim])\n",
    "\n",
    "D_W1 = tf.Variable(xavier_init([X_dim, h_dim]))\n",
    "D_b1 = tf.Variable(tf.zeros(shape=[h_dim]))\n",
    "\n",
    "D_W2 = tf.Variable(xavier_init([h_dim, 1]))\n",
    "D_b2 = tf.Variable(tf.zeros(shape=[1]))\n",
    "\n",
    "theta_D = [D_W1, D_W2, D_b1, D_b2]\n",
    "\n",
    "\n",
    "z = tf.placeholder(tf.float32, shape=[None, z_dim])\n",
    "\n",
    "G_W1 = tf.Variable(xavier_init([z_dim, h_dim]))\n",
    "G_b1 = tf.Variable(tf.zeros(shape=[h_dim]))\n",
    "\n",
    "G_W2 = tf.Variable(xavier_init([h_dim, X_dim]))\n",
    "G_b2 = tf.Variable(tf.zeros(shape=[X_dim]))\n",
    "\n",
    "theta_G = [G_W1, G_W2, G_b1, G_b2]\n",
    "\n",
    "def sample_z(m, n):    \n",
    "    return np.random.uniform(-1., 1., size=[m, n])\n",
    "\n",
    "\n",
    "def generator(z):\n",
    "    G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)\n",
    "    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2\n",
    "    #G_prob = tf.nn.sigmoid(G_log_prob)\n",
    "    return G_log_prob\n",
    "\n",
    "\n",
    "def discriminator(x):\n",
    "    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)\n",
    "    out = tf.matmul(D_h1, D_W2) + D_b2\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "G_sample = generator(z)\n",
    "D_real = discriminator(X)\n",
    "D_fake = discriminator(G_sample)\n",
    "\n",
    "\n",
    "\n",
    "# eps = tf.random_uniform([mb_size, 1], minval=0., maxval=1.)\n",
    "# X_inter = eps*X + (1. - eps)*G_sample\n",
    "# grad = tf.gradients(D(X_inter), [X_inter])[0]\n",
    "# grad_norm = tf.sqrt(tf.reduce_sum((grad)**2, axis=1))\n",
    "# grad_pen = lam * tf.reduce_mean((grad_norm - 1)**2)\n",
    "\n",
    "\n",
    "D_loss = -tf.reduce_mean(D_real) + tf.reduce_mean(D_fake)\n",
    "G_loss = -tf.reduce_mean(D_fake)\n",
    "\n",
    "D_solver = (tf.train.RMSPropOptimizer(learning_rate=1e-4)\n",
    "            .minimize(D_loss, var_list=theta_D))\n",
    "G_solver = (tf.train.RMSPropOptimizer(learning_rate=1e-4)\n",
    "            .minimize(G_loss, var_list=theta_G))\n",
    "\n",
    "clip_D = [p.assign(tf.clip_by_value(p, -0.01, 0.01)) for p in theta_D]\n",
    "d = tf.summary.scalar('D_loss', D_loss)\n",
    "g = tf.summary.scalar('G_loss', G_loss)\n",
    "                                      \n",
    "\n",
    "merged_d = tf.summary.merge(inputs=[d])\n",
    "merged_g = tf.summary.merge(inputs=[g])\n",
    "\n",
    "train_writer_g = tf.summary.FileWriter(\"tensorboard_FOG\" + '/train/gan_g')\n",
    "train_writer_d = tf.summary.FileWriter(\"tensorboard_FOG\" + '/train/gan_d')\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "if not os.path.exists('out/'):\n",
    "    os.makedirs('out/')\n",
    "# Create a saver.\n",
    "saver = tf.train.Saver( tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES))\n",
    "i = 0\n",
    "start = time.time()\n",
    "print(\"starting time:\", start)\n",
    "\n",
    "for it in range(1000001):\n",
    "    for _ in range(5):\n",
    "        X_mb, _, _ = fog.next_batch(\"gan\")\n",
    "        X_ = X_mb.reshape((-1,250*6))\n",
    "            \n",
    "        _, D_loss_curr, _, summary_ = sess.run(\n",
    "            [D_solver, D_loss, clip_D,merged_d],\n",
    "            feed_dict={X: X_, z: sample_z(mb_size, z_dim)}\n",
    "        )\n",
    "    train_writer_d.add_summary(summary_, it * 32 )\n",
    "    \n",
    "    _, G_loss_curr, _summary = sess.run([G_solver, G_loss, merged_g],\n",
    "        feed_dict={z: sample_z(mb_size, z_dim)})\n",
    "    \n",
    "    train_writer_g.add_summary(_summary, it * 32 )\n",
    "    \n",
    "    \n",
    "    if it % 100 == 0:\n",
    "        print('Iter: {}; D loss: {:.4}; G_loss: {:.4}'\n",
    "              .format(it, D_loss_curr, G_loss_curr))\n",
    "\n",
    "        if it % 1000 == 0:\n",
    "            print(\"current run time:\",time.time()-start)\n",
    "            samples = sess.run(G_sample, feed_dict={z: sample_z(256, z_dim)})\n",
    "            saver.save(sess, 'model_save/model_' + model_name, global_step=it)\n",
    "            with open(f\"out/sample-{it}.pickle\",\"wb\") as f:\n",
    "                pickle.dump(samples,f)\n",
    "            \n",
    "            #ig = plot(samples,it)\n",
    "            #plt.savefig('out/{}.png'\n",
    "             #           .format(str(i).zfill(3)), bbox_inches='tight')\n",
    "            i += 1\n",
    "            \n",
    "#plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
